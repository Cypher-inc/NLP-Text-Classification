
# ğŸ¬ NLP Text Classification Project

Harness the power of Natural Language Processing and Machine Learning to classify text data using traditional ML algorithms. This project walks through everything from cleaning raw HTML-laden text to building performant models and visualizing word patterns.

---

## ğŸ“Œ Overview

This notebook-based project focuses on **text classification** using popular machine learning models. It covers the complete NLP workflow:

* ğŸ§¹ Cleaning & preprocessing
* ğŸ§  Feature engineering with Bag of Words & TF-IDF
* ğŸ¤– Training ML models (Naive Bayes, Random Forest, Logistic Regression, XGBoost)
* ğŸ“ˆ Evaluating and comparing model performance

---

## ğŸ—‚ï¸ Project Flow

### ğŸ”¹ 1. Data Preprocessing

* Remove HTML tags
* Strip punctuation
* Generate word clouds for visualization

### ğŸ”¹ 2. Feature Engineering

* Train-Test split
* Bag of Words vectorization
* TF-IDF transformation

### ğŸ”¹ 3. Model Building

* ğŸ§ª **Naive Bayes**
* ğŸŒ² **Random Forest**
* ğŸ“Š **Logistic Regression**
* âš¡ **XGBoost**

### ğŸ”¹ 4. Evaluation

Compare training and testing accuracy, and detect underfitting or overfitting.

---

## ğŸ§ª Model Comparison

| ğŸ” Model                | âœ… Test Accuracy | ğŸ‹ï¸â€â™‚ï¸ Train Accuracy | ğŸ“Œ Notes                |
| ----------------------- | --------------- | --------------------- | ----------------------- |
| **Naive Bayes**         | 86%             | 86%                   | âœ”ï¸ Balanced performance |
| **Random Forest**       | 82%             | 83%                   | âš ï¸ Slight underfitting  |
| **Logistic Regression** | 82%             | 91%                   | âš ï¸ Overfitting detected |
| **XGBoost**             | 86%             | 91%                   | âš ï¸ Slight overfitting   |

---

## ğŸ› ï¸ Installation

Make sure you have Python 3.x installed. Then install the dependencies:

```bash
pip install -r requirements.txt
```

---

## ğŸš€ Running the Project

### â–¶ï¸ Option 1: Run in Jupyter Notebook

```bash
jupyter notebook "nlp.ipynb"
```

### â–¶ï¸ Option 2: Convert and run as script

```bash
jupyter nbconvert --to script "nlp.ipynb"
python nlp.py
```

---

## ğŸ“Œ Conclusion

âœ¨ **Key Takeaway**:
Naive Bayes and XGBoost achieved the highest test accuracy of **86%**.
Random Forest slightly underfit, while Logistic Regression overfit the training data.
